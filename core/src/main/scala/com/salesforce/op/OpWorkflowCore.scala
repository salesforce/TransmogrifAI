/*
 * Copyright (c) 2017, Salesforce.com, Inc.
 * All rights reserved.
 */

package com.salesforce.op

import com.salesforce.op.features.OPFeature
import com.salesforce.op.features.types.FeatureType
import com.salesforce.op.readers.{CustomReader, Reader, ReaderKey}
import com.salesforce.op.stages.impl.selector.{HasTestEval, ModelSelectorBase}
import com.salesforce.op.stages.{FeatureGeneratorStage, OPStage, OpTransformer}
import com.salesforce.op.utils.spark.RichDataset._
import org.apache.spark.ml._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.encoders.RowEncoder
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}
import org.slf4j.LoggerFactory

import scala.collection.mutable.{ArrayBuffer, ListBuffer}
import scala.reflect.runtime.universe.WeakTypeTag
import scala.util.Try


/**
 * Parameters for pipelines and pipeline models
 */
private[op] trait OpWorkflowCore {

  @transient protected lazy val log = LoggerFactory.getLogger(this.getClass)

  // the uid of the stage
  def uid: String

  // unique stages layered by distance (desc order)
  private[op] type StagesDAG = Array[Array[(OPStage, Int)]]

  // the data reader for the workflow or model
  private[op] var reader: Option[Reader[_]] = None

  // final features from workflow, used to find stages of the workflow
  private[op] var resultFeatures: Array[OPFeature] = Array[OPFeature]()

  // raw features generated after data is read in and aggregated
  private[op] var rawFeatures: Array[OPFeature] = Array[OPFeature]()

  // stages of the workflow
  private[op] var stages: Array[OPStage] = Array[OPStage]()

  // command line parameters for the workflow stages and readers
  private[op] var parameters = new OpParams()

  private[op] def setStages(value: Array[OPStage]): this.type = {
    stages = value
    this
  }

  private[op] def getStages(): Array[OPStage] = stages

  private[op] final def setRawFeatures(features: Array[OPFeature]): this.type = {
    rawFeatures = features
    this
  }

  /**
   * Set data reader that will be used to generate data frame for stages
   *
   * @param r reader for workflow
   * @return this workflow
   */
  final def setReader(r: Reader[_]): this.type = {
    reader = Option(r)
    checkUnmatchedFeatures()
    this
  }

  /**
   * Set input dataset which contains columns corresponding to the raw features used in the workflow
   * The type of the dataset (Dataset[T]) must match the type of the FeatureBuilders[T] used to generate
   * the raw features
   *
   * @param ds input dataset for workflow
   * @param key key extract function
   * @return this workflow
   */
  final def setInputDataset[T: WeakTypeTag](ds: Dataset[T], key: T => String = ReaderKey.randomKey _): this.type = {
    val newReader = new CustomReader[T](key) {
      def readFn(params: OpParams)(implicit spark: SparkSession): Either[RDD[T], Dataset[T]] = Right(ds)
    }
    reader = Option(newReader)
    checkUnmatchedFeatures()
    this
  }

  /**
   * Set input rdd which contains columns corresponding to the raw features used in the workflow
   * The type of the rdd (RDD[T]) must match the type of the FeatureBuilders[T] used to generate the raw features
   *
   * @param rdd input rdd for workflow
   * @param key key extract function
   * @return this workflow
   */
  final def setInputRDD[T: WeakTypeTag](rdd: RDD[T], key: T => String = ReaderKey.randomKey _): this.type = {
    val newReader = new CustomReader[T](key) {
      def readFn(params: OpParams)(implicit spark: SparkSession): Either[RDD[T], Dataset[T]] = Left(rdd)
    }
    reader = Option(newReader)
    checkUnmatchedFeatures()
    this
  }

  /**
   * Get the final features generated by the workflow
   *
   * @return result features for workflow
   */
  final def getResultFeatures(): Array[OPFeature] = resultFeatures

  /**
   * Get the parameter settings passed into the workflow
   *
   * @return OpWorkflowParams set for this workflow
   */
  final def getParameters(): OpParams = {
    parameters
  }

  /**
   * Determine if any of the raw features do not have a matching reader
   */
  protected def checkUnmatchedFeatures(): Unit = {
    if (rawFeatures.nonEmpty && reader.nonEmpty) {
      val readerInputTypes = reader.get.subReaders.map(_.fullTypeName).toSet
      val unmatchedFeatures = rawFeatures.filterNot(f =>
        readerInputTypes
          .contains(f.originStage.asInstanceOf[FeatureGeneratorStage[_, _ <: FeatureType]].tti.tpe.toString)
      )
      require(
        unmatchedFeatures.isEmpty,
        s"No matching data readers for ${unmatchedFeatures.length} input features:" +
          s" ${unmatchedFeatures.mkString(",")}. Readers had types: ${readerInputTypes.mkString(",")}"
      )
    }
  }

  /**
   * Used to generate dataframe from reader and raw features list
   *
   * @return Dataframe with all the features generated + persisted
   */
  protected def generateRawData()(implicit spark: SparkSession): DataFrame = {
    require(reader.nonEmpty, "Data reader must be set")
    require(rawFeatures.nonEmpty, "Result features must be set")

    checkUnmatchedFeatures()

    val subReaderTypes = reader.get.subReaders.map(_.typeName).toSet
    val unmatchedReaders = subReaderTypes.filterNot{ t => parameters.readerParams.contains(t) }

    if (unmatchedReaders.nonEmpty) {
      log.info(
        "Readers for types: {} do not have an override path in readerParams, so the default will be used",
        unmatchedReaders.mkString(","))
    }

    reader.get.generateDataFrame(rawFeatures, parameters).persist() // don't want to redo this
  }

  /**
   * Fit the estimators to return a sequence of only transformers
   * Modified version of Spark 2.x Pipeline
   *
   * @param data   dataframe to fit on
   * @return fitted model
   */
  protected def fitStages(data: DataFrame, stagesToFit: Array[OPStage])
    (implicit spark: SparkSession): Array[OPStage] = {

    // TODO may want to make workflow take an optional reserve fraction
    val splitters = stagesToFit.collect{ case s: ModelSelectorBase[_] => s.splitter }.flatten
    val splitter = splitters.reduceOption{ (a, b) => if (a.getReserveTestFraction > b.getReserveTestFraction) a else b }
    val (train, test) = splitter.map(_.split(data)).getOrElse{ (data, spark.emptyDataFrame) }
    val hasTest = !test.isEmpty


    // Search for the last estimator
    val indexOfLastEstimator = stagesToFit.view.zipWithIndex
      .collect { case (stage: Estimator[_], index) => index }
      .lastOption.getOrElse(-1)

    val transformers = ListBuffer.empty[Transformer]
    stagesToFit.view.zipWithIndex.foldLeft((train.toDF(), test.toDF())) {
      case ((currTrain, currTest), (stage, index)) =>
      if (index <= indexOfLastEstimator) { // only need to check for estimators before the last one
        val transformer: Transformer = stage match {
          case estimator: Estimator[_] =>
            estimator.fit(currTrain) match {
              case m: HasTestEval =>
                if (hasTest) m.evaluateModel(currTest)
                m
              case m => m
            }
          case t: Transformer => t
          case _ => throw new IllegalArgumentException(s"Does not support stage $stage of type ${stage.getClass}")
        }
        transformers += transformer
        if (index < indexOfLastEstimator) { // only need to update for fit before last estimator
          transformer.transform(currTrain) -> ( if (hasTest) transformer.transform(currTest) else currTest )
        } else {
          currTrain -> currTest
        }
      } else {
        transformers += stage.asInstanceOf[Transformer]
        currTrain -> currTest
      }
    }

    transformers.map(_.asInstanceOf[OPStage]).toArray
  }


  /**
   * Returns a Dataframe containing all the columns generated up to the stop stage
   *
   * @return Dataframe containing columns corresponding to all of the features generated before the feature given
   */
  protected def computeDataUpTo(stopStage: Option[Int], fitted: Boolean)(implicit spark: SparkSession): DataFrame = {
    if (stopStage.isEmpty) {
      log.warn("Could not find origin stage for feature in workflow!! Defaulting to generate raw features.")
      generateRawData()
    } else {
      val featureStages = stages.slice(0, stopStage.get)
      log.info("Found parent stage and computing features up to that stage:\n{}",
        featureStages.map(s => s.uid + " --> " + s.outputName).mkString("\n")
      )
      val rawData = generateRawData()

      if (!fitted) {
        val stages = fitStages(rawData, featureStages).map(_.asInstanceOf[Transformer])
        applySparkTransformations(rawData, stages, OpWorkflowModel.persistEveryKStages) // TODO use DAG transform
      } else {
        featureStages.foldLeft(rawData)((data, stage) => stage.asInstanceOf[Transformer].transform(data))
      }
    }
  }

  /**
   * Returns a dataframe containing all the columns generated up to the feature input
   *
   * @param feature input feature to compute up to
   * @return Dataframe containing columns corresponding to all of the features generated before the feature given
   */
  def computeDataUpTo(feature: OPFeature)(implicit spark: SparkSession): DataFrame

  /**
   * Computes a dataframe containing all the columns generated up to the feature input and saves it to the
   * specified path in avro format
   */
  def computeDataUpTo(feature: OPFeature, path: String)(implicit spark: SparkSession): Unit = {
    val df = computeDataUpTo(feature)
    df.saveAvro(path)
  }

  /**
   * Computes stages DAG
   *
   * @return unique stages layered by distance (desc order)
   */
  protected def computeStagesDAG(features: Array[OPFeature]): StagesDAG = {
    val (failures, parents) = features.map(_.parentStages()).partition(_.isFailure)

    if (failures.nonEmpty) {
      throw new IllegalArgumentException("Failed to compute stages DAG", failures.head.failed.get)
    }

    // Stages sorted by distance
    val sortedByDistance: Array[(OPStage, Int)] = parents.flatMap(_.get)

    // Stages layered by distance
    val layeredByDistance: Array[Array[(OPStage, Int)]] =
      sortedByDistance.groupBy(_._2).toArray
        .map(_._2.sortBy(_._1.outputName))
        .sortBy(s => -s.head._2)

    // Unique stages layered by distance
    layeredByDistance
      .foldLeft(Set.empty[OPStage], Array.empty[Array[(OPStage, Int)]]) {
        case ((seen, filtered), uncleaned) =>
          // filter out any seen stages. also add distinct to filter out any duplicate stages in layer
          val unseen = uncleaned.filterNot(v => seen.contains(v._1)).distinct
          val nowSeen = seen ++ unseen.map(_._1)
          (nowSeen, filtered :+ unseen)
      }._2
  }

  /**
   * Efficiently applies all fited stages grouping by level in the DAG where possible
   * @param rawData data to transform
   * @param dag computation graph
   * @param persistEveryKStages breaks in computation to persist
   * @param spark spark session
   * @return transformed dataframe
   */
  protected def applyTransformationsDAG(
    rawData: DataFrame, dag: StagesDAG, persistEveryKStages: Int
  )(implicit spark: SparkSession): DataFrame = {
    // A holder for the last persisted rdd
    var lastPersisted: Option[RDD[_]] = None

    // Apply stages layer by layer
    dag.foldLeft(rawData) { case (df, stagesLayer) =>
      // Apply all OP stages
      val opStages = stagesLayer.collect { case (s: OpTransformer, _) => s }
      val dfTransformed: DataFrame =
        if (opStages.isEmpty) df
        else {
          log.info("Applying {} OP stage(s): {}", opStages.length, opStages.map(_.uid).mkString(","))

          val newSchema = opStages.foldLeft(df.schema) {
            case (schema, s) =>
              s.setInputSchema(schema)
              s.transformSchema(schema)
          }
          val transforms = opStages.map(_.transformRow)
          val transformed: RDD[Row] =
            df.rdd.map { (row: Row) =>
              val values = new ArrayBuffer[Any](row.length + transforms.length)
              var i = 0
              while (i < row.length) {
                values += row.get(i)
                i += 1
              }
              for {transform <- transforms} values += transform(row)
              Row.fromSeq(values)
            }.persist()

          lastPersisted.foreach(_.unpersist())
          lastPersisted = Some(transformed)

          spark.createDataFrame(transformed, newSchema)
        }

      // Apply all non OP stages (ex. Spark wrapping stages etc)
      val sparkStages = stagesLayer.collect {
        case (s: Transformer, _) if !s.isInstanceOf[OpTransformer] => s.asInstanceOf[Transformer]
      }
      applySparkTransformations(dfTransformed, sparkStages, persistEveryKStages)
    }
  }

  /**
   * Transform the data using the specified Spark transformers.
   * Applying all the transformers one by one as [[org.apache.spark.ml.Pipeline]] does.
   *
   * ATTENTION: This method applies transformers sequentially (as [[org.apache.spark.ml.Pipeline]] does)
   * and usually results in slower run times with large amount of transformations due to Catalyst crashes,
   * therefore always remember to set 'persistEveryKStages' to break up Catalyst.
   *
   * @param transformers        spark transformers to apply
   * @param persistEveryKStages how often to break up Catalyst by persisting the data,
   *                            to turn off set to Int.MaxValue (not recommended)
   * @return Dataframe transformed data
   */
  protected def applySparkTransformations(
    data: DataFrame, transformers: Array[Transformer], persistEveryKStages: Int
  )(implicit spark: SparkSession): DataFrame = {

    // you have more than 5 stages and are not persisting at least once
    if (transformers.length > 5 && persistEveryKStages > transformers.length) {
      log.warn(
        "Number of transformers for scoring pipeline exceeds the persistence frequency. " +
          "Scoring performance may significantly degrade due to Catalyst optimizer issues. " +
          s"Consider setting 'persistEveryKStages' to a smaller number (ex. ${OpWorkflowModel.persistEveryKStages}).")
    }

    // A holder for the last persisted rdd
    var lastPersisted: Option[RDD[_]] = None

    // Apply all the transformers one by one as [[org.apache.spark.ml.Pipeline]] does
    val transformedData: DataFrame =
      transformers.zipWithIndex.foldLeft(data) { case (df, (stage, i)) =>
        val persist = i > 0 && i % persistEveryKStages == 0
        log.info(s"Applying stage: ${stage.uid}{}", if (persist) " (persisted)" else "")
        val newDF = stage.asInstanceOf[Transformer].transform(df)
        if (!persist) newDF
        else {
          // Converting to rdd and back here to break up Catalyst [SPARK-13346]
          val persisted = newDF.rdd.persist()
          lastPersisted.foreach(_.unpersist())
          lastPersisted = Some(persisted)
          spark.createDataFrame(persisted, newDF.schema)
        }
      }
    transformedData
  }

  /**
   * Looks at model parents to match parent stage for features (since features are created from the estimator not
   * the fitted transformer)
   *
   * @param feature feature want to find origin stage for
   * @return index of the parent stage
   */
  protected def findOriginStageId(feature: OPFeature): Option[Int] = Try {
    stages.map(_.outputName).indexOf(feature.name)
  }.toOption.filter(_ >= 0)

}
