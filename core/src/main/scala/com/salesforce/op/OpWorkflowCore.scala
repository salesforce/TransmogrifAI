/*
 * Copyright (c) 2017, Salesforce.com, Inc.
 * All rights reserved.
 */

package com.salesforce.op

import com.salesforce.op.features.OPFeature
import com.salesforce.op.features.types.FeatureType
import com.salesforce.op.readers.{CustomReader, DataReaders, Reader}
import com.salesforce.op.stages.{FeatureGeneratorStage, OPStage}
import com.salesforce.op.utils.spark.RichDataset._
import org.apache.spark.ml._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}
import org.slf4j.LoggerFactory

import scala.reflect.runtime.universe.WeakTypeTag
import scala.util.Try


/**
 * Parameters for pipelines and pipeline models
 */
private[op] trait OpWorkflowCore {

  @transient protected lazy val log = LoggerFactory.getLogger(this.getClass)

  // the uid of the stage
  def uid: String

  // the data reader for the workflow or model
  private[op] var reader: Option[Reader[_]] = None

  // final features from workflow, used to find stages of the workflow
  private[op] var resultFeatures: Array[OPFeature] = Array[OPFeature]()

  // raw features generated after data is read in and aggregated
  private[op] var rawFeatures: Array[OPFeature] = Array[OPFeature]()

  // stages of the workflow
  private[op] var stages: Array[OPStage] = Array[OPStage]()

  // command line parameters for the workflow stages and readers
  private[op] var parameters = new OpParams()

  private[op] def setStages(value: Array[OPStage]): this.type = {
    stages = value
    this
  }

  private[op] def getStages(): Array[OPStage] = stages

  private[op] final def setRawFeatures(features: Array[OPFeature]): this.type = {
    rawFeatures = features
    this
  }

  /**
   * Set data reader that will be used to generate data frame for stages
   *
   * @param r reader for workflow
   * @return this workflow
   */
  final def setReader(r: Reader[_]): this.type = {
    reader = Option(r)
    checkUnmatchedFeatures()
    this
  }

  /**
   * Set input dataset which contains columns corresponding to the raw features used in the workflow
   * The type of the dataset (Dataset[T]) must match the type of the FeatureBuilders[T] used to generate
   * the raw features
   *
   * @param ds input dataset for workflow
   * @param key key extract function
   * @return this workflow
   */
  final def setInputDataset[T: WeakTypeTag](ds: Dataset[T], key: T => String = DataReaders.randomKey _): this.type = {
    val newReader = new CustomReader[T](key) {
      def readFn(params: OpParams)(implicit spark: SparkSession): Either[RDD[T], Dataset[T]] = Right(ds)
    }
    reader = Option(newReader)
    checkUnmatchedFeatures()
    this
  }

  /**
   * Set input rdd which contains columns corresponding to the raw features used in the workflow
   * The type of the rdd (RDD[T]) must match the type of the FeatureBuilders[T] used to generate the raw features
   *
   * @param rdd input rdd for workflow
   * @param key key extract function
   * @return this workflow
   */
  final def setInputRDD[T: WeakTypeTag](rdd: RDD[T], key: T => String = DataReaders.randomKey _): this.type = {
    val newReader = new CustomReader[T](key) {
      def readFn(params: OpParams)(implicit spark: SparkSession): Either[RDD[T], Dataset[T]] = Left(rdd)
    }
    reader = Option(newReader)
    checkUnmatchedFeatures()
    this
  }


  /**
   * Get the final features generated by the workflow
   *
   * @return result features for workflow
   */
  final def getResultFeatures(): Array[OPFeature] = resultFeatures


  /**
   * Get the parameter settings passed into the workflow
   *
   * @return OpWorkflowParams set for this workflow
   */
  final def getParameters(): OpParams = {
    parameters
  }

  /**
   * Determine if any of the raw features do not have a matching reader
   */
  protected def checkUnmatchedFeatures(): Unit = {
    if (rawFeatures.nonEmpty && reader.nonEmpty) {
      val readerInputTypes = reader.get.subReaders.map(_.fullTypeName).toSet
      val unmatchedFeatures = rawFeatures.filterNot(f =>
        readerInputTypes
          .contains(f.originStage.asInstanceOf[FeatureGeneratorStage[_, _ <: FeatureType]].tti.tpe.toString)
      )
      require(
        unmatchedFeatures.isEmpty,
        s"No matching data readers for ${unmatchedFeatures.length} input features:" +
          s" ${unmatchedFeatures.mkString(",")}. Readers had types: ${readerInputTypes.mkString(",")}"
      )
    }
  }

  /**
   * Used to generate dataframe from reader and raw features list
   *
   * @param spark
   * @return
   */
  protected def generateRawData()(implicit spark: SparkSession): DataFrame = {
    require(reader.nonEmpty, "Data reader must be set")
    require(rawFeatures.nonEmpty, "Result features must be set")

    checkUnmatchedFeatures()

    val subReaderTypes = reader.get.subReaders.map(_.typeName).toSet
    val unmatchedReaders = subReaderTypes.filterNot{ t => parameters.readerParams.contains(t) }

    if (unmatchedReaders.nonEmpty) {
      log.info(
        "Readers for types: {} do not have an override path in readerParams, so the default will be used",
        unmatchedReaders.mkString(","))
    }

    reader.get.generateDataFrame(rawFeatures, parameters).persist() // don't want to redo this
  }

  /**
   * Returns a Dataframe containing all the columns generated up to the stop stage
   *
   * @return Dataframe containing columns corresponding to all of the features generated before the feature given
   */
  protected def computeDataUpTo(stopStage: Option[Int], fitted: Boolean)(implicit spark: SparkSession): DataFrame = {
    if (stopStage.isEmpty) {
      log.warn("Could not find origin stage for feature in workflow!! Defaulting to generate raw features.")
      generateRawData()
    } else {
      val featureStages = stages.slice(0, stopStage.get)
      log.info("Found parent stage and computing features up to that stage:\n{}",
        featureStages.map(s => s.uid + " --> " + s.outputName).mkString("\n")
      )
      val rawData = generateRawData()

      if (!fitted) {
        val fitPipeline = sparkPipelineFit(stages = featureStages.map(_.asInstanceOf[PipelineStage]), data = rawData)
        fitPipeline.transform(rawData)
      } else {
        featureStages.foldLeft(rawData)((data, stage) => stage.asInstanceOf[Transformer].transform(data))
      }
    }
  }

  /**
   * Returns a dataframe containing all the columns generated up to the feature input
   *
   * @param feature input feature to compute up to
   * @return Dataframe containing columns corresponding to all of the features generated before the feature given
   */
  def computeDataUpTo(feature: OPFeature)(implicit spark: SparkSession): DataFrame

  /**
   * Computes a dataframe containing all the columns generated up to the feature input and saves it to the
   * specified path in avro format
   */
  def computeDataUpTo(feature: OPFeature, path: String)(implicit spark: SparkSession): Unit = {
    val df = computeDataUpTo(feature)
    df.saveAvro(path)
  }

  /**
   * Compute stages DAG
   *
   * @return unique stages layered by distance (desc order)
   */
  protected def computeStagesDAG(features: Array[OPFeature]): Array[Array[(OPStage, Int)]] = {
    val (failures, parents) = features.map(_.parentStages()).partition(_.isFailure)

    if (failures.nonEmpty) {
      throw new IllegalArgumentException("Failed to compute stages DAG", failures.head.failed.get)
    }

    // Stages sorted by distance
    val sortedByDistance: Array[(OPStage, Int)] = parents.flatMap(_.get)

    // Stages layered by distance
    val layeredByDistance: Array[Array[(OPStage, Int)]] =
      sortedByDistance.groupBy(_._2).toArray
        .map(_._2.sortBy(_._1.outputName))
        .sortBy(s => -s.head._2)

    // Unique stages layered by distance
    layeredByDistance
      .foldLeft(Set.empty[OPStage], Array.empty[Array[(OPStage, Int)]]) {
        case ((seen, filtered), uncleaned) =>
          // filter out any seen stages. also add distinct to filter out any duplicate stages in layer
          val unseen = uncleaned.filterNot(v => seen.contains(v._1)).distinct
          val nowseen = seen ++ unseen.map(_._1)
          (nowseen, filtered :+ unseen)
      }._2
  }

  /**
   * Looks at model parents to match parent stage for features (since features are created from the estimator not
   * the fitted transformer)
   *
   * @param feature feature want to find origin stage for
   * @return index of the parent stage
   */
  protected def findOriginStageId(feature: OPFeature): Option[Int] = Try {
    stages.map(_.outputName).indexOf(feature.name)
  }.toOption.filter(_ >= 0)

  /**
   * Fit the Spark pipeline
   *
   * @param stages pipeline stages
   * @param data   dataframe to fit on
   * @return fitted model
   */
  protected def sparkPipelineFit(
    stages: Array[_ <: PipelineStage] = stages.map(_.asInstanceOf[PipelineStage]),
    data: DataFrame
  ): PipelineModel = {
    new Pipeline(uid).setStages(stages).fit(data)
  }

}
