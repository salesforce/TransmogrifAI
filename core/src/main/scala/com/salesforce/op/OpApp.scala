/*
 * Copyright (c) 2017, Salesforce.com, Inc.
 * All rights reserved.
 */

package com.salesforce.op

import com.salesforce.op.utils.kryo.OpKryoRegistrator
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession
import org.slf4j.LoggerFactory
import scopt.Read


/**
 * A simple command line app for running an [[OpWorkflow]] with Spark.
 * A user needs to implement a [[run]] function.
 */
abstract class OpApp {

  @transient private val log = LoggerFactory.getLogger(this.getClass)

  /**
   * The main function to run your [[OpWorkflow]].
   * The easiest way is to create an [[OpWorkflowRunner]] and run it.
   *
   * @param runType  run type
   * @param opParams op params
   * @param spark    spark session
   */
  def run(runType: OpWorkflowRunType, opParams: OpParams)(implicit spark: SparkSession): Unit

  /**
   * Kryo registrar to use when creating a SparkConf.
   *
   * First create your own registrator by extending the [[OpKryoRegistrator]]
   * and then register your new classes by overriding [[OpKryoRegistrator.registerCustomClasses]].
   *
   * Then override this method to set your registrator with Spark.
   */
  def kryoRegistrator: Class[_ <: OpKryoRegistrator] = classOf[OpKryoRegistrator]

  /**
   * Application name (default: Class simple name).
   */
  def appName: String = this.getClass.getSimpleName.stripSuffix("$")

  /**
   * Configuration for a Spark application.
   * Used to set various Spark parameters as key-value pairs.
   *
   * @return SparkConf
   */
  def sparkConf: SparkConf = {
    new SparkConf()
      .setAppName(appName)
      .set("spark.serializer", classOf[org.apache.spark.serializer.KryoSerializer].getCanonicalName)
      .set("spark.kryo.registrator", kryoRegistrator.getCanonicalName)
    // .set("spark.ui.enabled", "false") // Disables Spark Application UI
    // .set("spark.kryo.registrationRequired", "true") // Enable to debug Kryo
    // .set("spark.kryo.unsafe", "true") // This might improve performance
  }

  /**
   * Gets/creates a Spark Session.
   */
  def sparkSession: SparkSession = {
    val conf = sparkConf
    if (log.isDebugEnabled) {
      log.debug("*" * 80)
      log.debug("SparkConf:\n{}", conf.toDebugString)
      log.debug("*" * 80)
    }
    SparkSession.builder.config(conf).getOrCreate()
  }

  /**
   * Parse command line arguments as [[OpParams]].
   *
   * @param args command line arguments
   * @return run type and [[OpParams]]
   */
  def parseArgs(args: Array[String]): (OpWorkflowRunType, OpParams) = {
    def optStr(s: String): Option[String] = if (s == null || s.isEmpty) None else Some(s)

    val parser = new scopt.OptionParser[OpWorkflowRunnerConfig](appName) {
      implicit val runTypeRead: Read[OpWorkflowRunType] = scopt.Read.reads(OpWorkflowRunType.withNameInsensitive)

      opt[OpWorkflowRunType]('t', "run-type").required().action { (x, c) =>
        c.copy(runType = x)
      }.text(s"the type of workflow run: ${OpWorkflowRunType.values.mkString(", ").toLowerCase}")

      opt[Map[String, String]]('r', "read-location").action { (x, c) =>
        c.copy(readLocations = x)
      }.text("optional location to read data from - will override reader default locations")

      opt[String]('m', "model-location").action { (x, c) =>
        c.copy(modelLocation = optStr(x))
      }.text("location to write/read a fitted model generated by workflow")

      opt[String]('w', "write-location").action { (x, c) =>
        c.copy(writeLocation = optStr(x))
      }.text("location in which to write out data generated by workflow")

      opt[String]('x', "metrics-location").action { (x, c) =>
        c.copy(metricsLocation = optStr(x))
      }.text("location in which to write out metrics generated by workflow")

      opt[String]('p', "param-location").action { (x, c) =>
        c.copy(paramLocation = optStr(x))
      }.text("optional location of parameters for workflow run")

      checkConfig(_.validate match { case Left(error: String) => Left(error) case _ => Right(()) })
      help("help").text("prints this usage text")
    }

    val config = parser.parse(args, OpWorkflowRunnerConfig())
    if (config.isEmpty) sys.exit(1)

    log.info("Parsed config:\n{}", config)
    config.get.runType -> config.get.toOpParams.get
  }

  /**
   * The main method - loads the params and runs the workflow according to parameter settings.
   *
   * @param args command line args to be parsed into [[OpWorkflowRunnerConfig]]
   */
  def main(args: Array[String]): Unit = {
    val (runType, opParams) = parseArgs(args)
    implicit val spark = sparkSession
    run(runType, opParams)
  }

}

/**
 * A simple command line app for running an [[OpWorkflow]] with Spark.
 * A user needs to implement a [[runner]] creation function.
 */
abstract class OpAppWithRunner extends OpApp {
  /**
   * Override this function to create an instance of [[OpWorkflowRunner]] to run your workflow
   *
   * @param opParams op params
   * @return an instance of [[OpWorkflowRunner]]
   */
  def runner(opParams: OpParams): OpWorkflowRunner

  /**
   * The main function to run your [[OpWorkflow]].
   * The easiest way is to create an [[OpWorkflowRunner]] and run it.
   *
   * @param runType  run type
   * @param opParams op params
   * @param spark    spark session
   */
  override def run(runType: OpWorkflowRunType, opParams: OpParams)(implicit spark: SparkSession): Unit = {
    runner(opParams).run(runType, opParams)
  }
}
