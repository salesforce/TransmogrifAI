/*
 * Copyright (c) 2017, Salesforce.com, Inc.
 * All rights reserved.
 */

package com.salesforce.op

import com.salesforce.op.features.OPFeature
import com.salesforce.op.stages.OPStage
import com.salesforce.op.utils.reflection.ReflectionUtils
import org.apache.spark.sql.{DataFrame, SparkSession}

import scala.util.{Failure, Try}


/**
 * Workflow for Optimus Prime. Takes the final features that the user wants to generate as inputs and
 * constructs the full DAG needed to generate them from those features lineage. Then fits any estimators in the
 * pipeline dag to create a sequence of transformations that are saved in a workflow model.
 *
 * @param uid unique id for the workflow
 */
class OpWorkflow(val uid: String = UID[OpWorkflow]) extends OpWorkflowCore {

  /**
   * Set stage and reader parameters from OpWorkflowParams object for run
   *
   * @param newParams new parameter values
   * @return this workflow
   */
  final def setParameters(newParams: OpParams): this.type = {
    parameters = newParams
    if (stages.nonEmpty) setStageParameters(stages)
    this
  }

  /**
   * This is used to set the stages of the workflow.
   *
   * By setting the final features the stages used to
   * generate them can be traced back through the parent features and origin stages.
   * The input is an tuple of features to support leaf feature generation (multiple endpoints in feature generation).
   *
   * @param features Final features generated by the workflow
   */
  def setResultFeatures(features: OPFeature*): this.type = {
    val featuresArr = features.toArray
    resultFeatures = featuresArr
    rawFeatures = featuresArr.flatMap(_.rawFeatures).distinct.sortBy(_.name)
    checkUnmatchedFeatures()
    setStages(features = featuresArr)
    validateStages()

    if (log.isDebugEnabled) {
      log.debug(s"\nDependency graphs resolved into a stage sequence of:\n{}",
        getStages().map(s =>
          s" ${s.uid}[${s.getInputFeatures().map(_.name).mkString(",")}] --> ${s.outputName}"
        ).mkString("\n")
      )
      log.debug("*" * 80)
      log.debug("Result features:")
      resultFeatures.foreach(feature => log.debug(s"${feature.name}:\n${feature.prettyParentStages}"))
      log.debug("*" * 80)
    }
    this
  }

  /**
   * Set parameters from stage params map unless param is set in code.
   * Note: Will NOT override parameter values that have been
   * set previously in code OR with a previous set of parameters.
   */
  private def setStageParameters(stages: Array[OPStage]): Unit = {
    for {
      (stageName, stageParams) <- parameters.stageParams
      stage <- stages.filter(s => s.getClass.getSimpleName == stageName || s.uid.startsWith(s"${stageName}_"))
      (k, v) <- stageParams
    } {
      val setStage = Try {
        val paramId = stage.getParam(k)
        if (!stage.isSet(paramId)) {
          stage.set(paramId, v)
          log.info(s"Set parameter $k to value $v for stage $stage")
        } else log.warn(
          s"Parameter $k was not set to value $v because it has already been set with value ${stage.get(paramId)}"
        )
      }
      if (setStage.isFailure) log.error(
        s"Setting parameter $k with value $v for stage $stage with params ${stage.params.toList} failed with an error",
        setStage.failed.get
      )
    }
  }

  /**
   * Uses input features to reconstruct the DAG of stages needed to generate them
   *
   * @param features final features passed into setInput
   */
  private def setStages(features: Array[OPFeature]): OpWorkflow.this.type = {

    // Unique stages layered by distance
    val uniqueStagesLayered: Array[Array[(OPStage, Int)]] = computeStagesDAG(features)

    if (log.isDebugEnabled) {
      val total = uniqueStagesLayered.map(_.length).sum
      val stages = for {
        layer <- uniqueStagesLayered
        (stage, distance) <- layer
      } yield s"$stage with distance $distance with output ${stage.getOutput().name}"

      log.debug("*" * 80)
      log.debug(s"Setting $total parent stages (sorted by distance desc):\n{}", stages.mkString("\n"))
      log.debug("*" * 80)
    }

    val uniqueStages: Array[OPStage] = uniqueStagesLayered.flatMap(_.map(_._1))

    setStageParameters(uniqueStages)
    setStages(uniqueStages)
  }

  /**
   * Transform function for testing chained transformations
   *
   * @param in DataFrame
   * @return transformed DataFrame
   */
  private[op] def transform(in: DataFrame): DataFrame = {
    sparkPipelineFit(data = in).transform(in)
  }

  /**
   * Check if all the stages of the workflow are serializable
   *
   * @return Failure if not serializable
   */
  private[op] def checkSerializable(): Try[Unit] = Try {
    val failures = stages.map(s => s.uid -> s.checkSerializable).collect { case (stageId, Failure(e)) => stageId -> e }

    if (failures.nonEmpty) throw new IllegalArgumentException(
      s"All stages must be serializable. Failed stages: ${failures.map(_._1).mkString(",")}",
      failures.head._2
    )
  }

  /**
   * Check if all the stages of the workflow have uid argument in their constructors
   * (required for workflow save/load to work)
   *
   * @return Failure if there is at least one stage without a uid argument in constructor
   */
  private[op] def checkCtorUIDs(): Try[Unit] = checkCtorArgs(arg = "uid")

  /**
   * Check if all the stages of the workflow have a specified argument 'arg' in their constructors
   * (required for workflow save/load to work)
   *
   * @param arg ctor argument to check
   * @return Failure if there is at least one stage without a 'arg' argument in constructor
   */
  private[op] def checkCtorArgs(arg: String): Try[Unit] = Try {
    val failures =
      stages.map(s => s.uid -> ReflectionUtils.bestCtorWithArgs(s)._2.map(_._1))
        .collect { case (stageId, args) if !args.contains(arg) => stageId }

    if (failures.nonEmpty) throw new IllegalArgumentException(
      s"All stages must be have $arg as their ctor argument. Failed stages: ${failures.mkString(",")}"
    )
  }

  /**
   * Check if all the stages of the workflow have uid argument in their constructors
   * (required for workflow save/load to work)
   *
   * @return Failure if there is at least one stage without a uid argument in constructor
   */
  private[op] def checkDistinctUIDs(): Try[Unit] = Try {
    if (stages.map(_.uid).distinct.length != stages.length) throw new IllegalArgumentException(
      "All stages must be distinct instances with distinct uids for saving"
    )
  }

  /**
   * Validate all the workflow stages
   *
   * @throws IllegalArgumentException
   */
  private[op] def validateStages(): Unit = {
    val res = for {
      _ <- checkCtorUIDs()
      _ <- checkSerializable()
      _ <- checkDistinctUIDs()
    } yield ()
    if (res.isFailure) throw res.failed.get
  }

  /**
   * Fit all of the estimators in the pipeline and return a pipeline model of only transformers. Uses data loaded
   * as specified by the data reader to generate the initial data set.
   *
   * @return a fitted pipeline model
   */
  def train()(implicit spark: SparkSession): OpWorkflowModel = {
    val rawData = generateRawData()
    val fitPipeline = sparkPipelineFit(data = rawData)

    // Update features with fitted stages
    val fittedStages = fitPipeline.stages.map(_.asInstanceOf[OPStage])
    val newResultFeatures = resultFeatures.map(_.copyWithNewStages(fittedStages))

    val model =
      new OpWorkflowModel(uid)
        .setParent(this)
        .setStages(fittedStages)
        .setFeatures(newResultFeatures)
        .setParameters(parameters)

    reader.map(model.setReader).getOrElse(model)
  }

  /**
   * Replaces any estimators in this workflow with their corresponding fit models from the OpWorkflowModel
   * passed in. Note that the Stages UIDs must EXACTLY correspond in order to be replaced so the same features
   * and stages must be used in both the fitted OpWorkflowModel and this OpWorkflow.
   * Any estimators that are not part of the OpWorkflowModel passed in will be trained when .train()
   * is called on this OpWorkflow.
   *
   * @param model model containing fitted stages to be used in this workflow
   * @return an OpWorkflow containing all of the stages from this model plus any new stages
   *         needed to generate the features not included in the fitted model
   */
  def withModelStages(model: OpWorkflowModel): this.type = {
    val newResultFeatures = (resultFeatures ++ model.getResultFeatures()).map(_.copyWithNewStages(model.stages))
    setResultFeatures(newResultFeatures: _*)
  }

  /**
   * Load a previously trained workflow model from path
   *
   * @param path to the trained workflow model
   * @return workflow model
   */
  def loadModel(path: String): OpWorkflowModel = new OpWorkflowModelReader(this).load(path)

  /**
   * Returns a dataframe containing all the columns generated up to the feature input
   *
   * @param feature input feature to compute up to
   * @return Dataframe containing columns corresponding to all of the features generated before the feature given
   */
  def computeDataUpTo(feature: OPFeature)(implicit spark: SparkSession): DataFrame = {
    computeDataUpTo(stopStage = findOriginStageId(feature), fitted = false)
  }

}
